{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders.emailTextLoader import get_emails\n",
    "from loaders.ytbChannelLoader import get_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email_corpus = get_emails()\n",
    "# video_corpus = get_videos()\n",
    "\n",
    "with open('test/day_mail_corpus.txt', 'r') as f:\n",
    "    email_corpus = f.read()\n",
    "\n",
    "with open('test/day_video_corpus.txt', 'r') as f:\n",
    "    video_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(email_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('test/day_mail_corpus.txt', 'w') as f:\n",
    "#     f.write(email_corpus)\n",
    "\n",
    "# with open('test/day_video_corpus.txt', 'w') as f:\n",
    "#     f.write(video_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "\n",
    "    # Define a regular expression pattern to match emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                            u\"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "                            u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes\n",
    "                            u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                            u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                            u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                            u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                            u\"\\U0001F004-\\U0001F0CF\"  # CJK Compatibility Ideographs\n",
    "                            u\"\\U0001F170-\\U0001F251\"  # Enclosed Ideographic Supplement\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    # Remove emojis from the text\n",
    "    text_without_emojis = re.sub(emoji_pattern, '', text)\n",
    "\n",
    "    return text_without_emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_emojis(email_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example preprocessed text data\n",
    "corpus = email_corpus.split('.')\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "k = 75  # Number of clusters (you can experiment with different values)\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualize the clusters using PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "# Scatter plot of clusters\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')\n",
    "plt.title(f'K-Means Clustering (k={k})')\n",
    "plt.show()\n",
    "\n",
    "# Create a dictionary to store representatives for each cluster\n",
    "cluster_representatives = {}\n",
    "\n",
    "# Iterate through data points and assign them to clusters\n",
    "for i, label in enumerate(clusters):\n",
    "    if label not in cluster_representatives:\n",
    "        cluster_representatives[label] = []\n",
    "    cluster_representatives[label].append(i)\n",
    "\n",
    "# Choose representatives (original text) for each cluster\n",
    "cluster_texts = {}\n",
    "for label, data_point_indices in cluster_representatives.items():\n",
    "    cluster_text = [corpus[i] for i in data_point_indices]\n",
    "    cluster_texts[label] = cluster_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text in cluster_texts.items():\n",
    "    print(f'Cluster {label}:')\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming(corpus):\n",
    "    # Download the NLTK data (if not already downloaded)\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Initialize the Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "\n",
    "    # Apply stemming to each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the stemmed words back into a sentence\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "\n",
    "    print(len(stemmed_text))\n",
    "\n",
    "    return stemmed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def lemma(corpus):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(corpus)\n",
    "\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    print(len(lemmatized_text))\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def remove_stop(corpus):\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped_words = [word.translate(table) for word in words]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in stripped_words if word.lower() not in stop_words]\n",
    "\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    print(len(filtered_text))\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming(email_corpus)\n",
    "lemma(email_corpus)\n",
    "remove_stop(email_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(email_corpus)\n",
    "\n",
    "# Calculate TF-IDF scores for words in sentences\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Calculate sentence importance scores based on TF-IDF\n",
    "sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "\n",
    "# Select the top N sentences\n",
    "num_sentences = 10  # Adjust this based on your desired summary length\n",
    "selected_sentences = []\n",
    "for i in range(num_sentences):\n",
    "    max_score_index = sentence_scores.argmax()\n",
    "    selected_sentences.append(sentences[max_score_index])\n",
    "    sentence_scores[max_score_index] = 0  # Mark the selected sentence as visited\n",
    "\n",
    "# Reconstruct the summary\n",
    "summary = ' '.join(selected_sentences)\n",
    "print(len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bardapi python-dotenv transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from bardapi import BardCookies\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class BardRAPI(LLM):\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        load_dotenv()\n",
    "        print(prompt)\n",
    "        bard = BardCookies(token_from_browser=True)\n",
    "        return bard.get_answer(prompt)['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "def summarize_text(corpora):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000\n",
    "    )\n",
    "    texts = text_splitter.split_text(corpora)\n",
    "    docs = [Document(page_content=t) for t in texts]\n",
    "\n",
    "    prompt_template = \"\"\"You are given a text about various news happening in the space of artifical intelligence, machine learning or data science.\n",
    "    Your job is to summarize the most important news from the text, with emphasis on news around large language models, and the tools that are used to handle them.\n",
    "    Don't worry about the length of the summary, just make sure it is coherent and covers the most important points, and also does not skimp on details.\n",
    "\n",
    "\n",
    "    {text}\n",
    "\n",
    "    CONCISE SUMMARY: \"\"\"\n",
    "\n",
    "    gptlm = BardRAPI()\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "    chain = load_summarize_chain(gptlm, chain_type=\"map_reduce\", map_prompt=PROMPT, verbose=True)\n",
    "    \n",
    "    print(chain.llm_chain.prompt.template)\n",
    "    print(chain.combine_document_chain.llm_chain.prompt.template)\n",
    "\n",
    "    output_summary = chain.run(docs)\n",
    "    return output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = summarize_text(remove_stop(email_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bardapi import BardCookies\n",
    "from datetime import datetime\n",
    "\n",
    "bard = BardCookies(token_from_browser=True)\n",
    "audio = bard.speech(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "today_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open(f\"summaries/audio/summary_{today_str}.ogg\", \"wb\") as f:\n",
    "  f.write(bytes(audio['audio']))\n",
    "\n",
    "with open(f\"summaries/text/summary_{today_str}.txt\", \"w\") as f:\n",
    "  f.write(response)\n",
    "\n",
    "print(response)\n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(f'summaries/text/summary_{today_str}.ogg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(\"git add summaries/text\", shell=True)\n",
    "subprocess.run(\"git add summaries/audio\", shell=True)\n",
    "\n",
    "subprocess.run(f\"git commit -m 'Summary for {today_str}'\", shell=True)\n",
    "subprocess.run(\"git push origin master\", shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
